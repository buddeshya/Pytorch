{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9079, 0.9571],\n",
      "        [0.1026, 0.0916]]) \n",
      "\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.1228, 0.0998, 0.2114],\n",
      "        [0.7049, 0.1203, 0.3467]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd is PyTorch’s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/uddesha-barnwal/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:07<00:00, 6.05MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "tensor([[-5.1890e-01, -1.1940e-01, -6.1929e-01, -1.4859e+00, -8.2792e-01,\n",
      "         -4.5718e-02, -2.8568e-01,  6.3355e-01,  6.2139e-01, -5.4454e-01,\n",
      "         -5.7090e-01, -4.9678e-01,  9.4472e-02, -6.9679e-01, -8.3539e-01,\n",
      "         -3.8976e-01, -5.7034e-01, -3.0209e-02, -2.8995e-01, -1.8936e-01,\n",
      "         -1.2652e+00, -2.3536e-01, -1.2335e+00,  3.3686e-01, -5.7827e-01,\n",
      "         -5.8334e-01, -3.0627e-01, -7.6905e-01, -4.6655e-01,  3.3846e-02,\n",
      "         -5.2051e-01, -4.3312e-01,  7.5469e-02, -3.4679e-01, -3.0756e-01,\n",
      "         -1.6165e-01,  8.7822e-01, -2.1851e-01, -1.2055e-01,  1.8715e-01,\n",
      "         -3.4367e-01, -2.7599e-01, -6.2219e-01,  2.6392e-02, -1.7419e-01,\n",
      "         -8.7399e-02, -4.8259e-01,  1.2486e-02, -1.0206e+00, -8.7713e-01,\n",
      "         -2.1318e-01,  6.1756e-01, -1.6805e-01, -4.7517e-01,  8.2847e-02,\n",
      "         -1.1407e+00, -3.4152e-01, -1.0240e+00, -2.4831e-01, -1.3588e-01,\n",
      "          9.5729e-01,  5.3178e-02,  1.3617e-01,  3.2600e-01, -4.5804e-01,\n",
      "         -1.8673e-01,  2.6759e-02, -2.8733e-02, -3.2445e-01, -1.0329e+00,\n",
      "         -1.4849e+00,  2.2936e-01, -1.3091e+00, -3.8574e-02, -9.7328e-01,\n",
      "         -1.1240e+00, -3.6051e-02, -5.3616e-01,  4.4733e-01, -8.8155e-02,\n",
      "         -1.4892e-01, -1.0300e+00,  3.7033e-01, -5.0238e-03, -1.3512e-01,\n",
      "          3.9718e-01,  5.2197e-01,  4.5857e-01,  4.4501e-02, -5.6338e-01,\n",
      "         -8.2999e-01, -1.0081e+00, -1.5664e+00, -6.1595e-02,  4.8255e-01,\n",
      "         -1.8286e+00, -7.2312e-01, -3.2846e-01, -1.3763e+00, -9.4944e-02,\n",
      "         -1.0896e+00, -1.0374e+00, -4.9564e-01, -1.8037e-01,  1.3616e-01,\n",
      "         -7.0705e-01, -3.9756e-02, -1.1655e+00, -9.5909e-01, -1.2482e+00,\n",
      "         -8.2408e-01, -3.8089e-01,  1.2000e+00,  6.1076e-01,  4.1306e-01,\n",
      "         -5.1600e-01, -4.1892e-01, -1.5984e-01,  7.5633e-01, -8.6453e-02,\n",
      "         -3.4876e-01,  2.1769e-01,  3.0805e-01,  1.2593e-01,  1.2216e+00,\n",
      "          2.0050e-01,  4.0763e-01, -1.5227e+00, -1.2666e+00, -1.3231e+00,\n",
      "         -1.5530e+00, -1.4195e+00, -1.0936e+00, -1.0113e+00, -4.6660e-01,\n",
      "         -1.0266e+00, -8.1143e-01, -1.1598e+00, -8.4722e-01, -1.1930e+00,\n",
      "         -1.2760e+00, -1.7557e+00, -2.0789e+00, -1.4518e+00, -7.2969e-01,\n",
      "         -3.2919e-01, -9.5565e-01, -1.6835e+00, -1.0784e+00, -1.0392e+00,\n",
      "          7.3618e-01,  1.6780e+00, -1.0380e+00, -6.9479e-01, -9.8625e-02,\n",
      "         -1.0328e-01, -5.5367e-01, -1.9881e-01,  4.1368e-01,  1.7956e-01,\n",
      "          2.4287e-01,  2.5385e-01, -6.1224e-02,  2.9604e-01,  2.8714e-01,\n",
      "         -3.2595e-01, -1.8784e-01, -3.7182e-01,  5.1549e-01, -4.7164e-01,\n",
      "          2.3788e-02,  9.1667e-01,  3.8041e-01,  3.5664e-01,  8.9231e-02,\n",
      "         -7.9140e-01, -3.6583e-02,  9.2352e-02,  6.6931e-01,  6.7496e-01,\n",
      "          5.4409e-01, -1.0923e-01,  1.6532e-01,  1.0153e-03,  3.4044e-01,\n",
      "          3.6432e-01,  4.0881e-01,  8.1746e-02,  1.9494e-01,  3.5531e-01,\n",
      "         -8.0798e-01,  1.7803e-01,  3.2925e-01,  4.9715e-01, -1.0176e+00,\n",
      "          6.7646e-01, -2.6839e-01, -9.6783e-02, -5.3029e-02,  4.6207e-01,\n",
      "         -1.1277e-02,  3.2956e-02,  1.3108e-01,  5.3137e-01, -3.7196e-01,\n",
      "          2.2790e-01,  4.3070e-02,  4.3088e-01,  1.2891e+00,  4.0118e-01,\n",
      "          1.6319e-01,  5.0955e-01,  2.9773e-01, -8.5461e-02, -2.1893e-01,\n",
      "          3.8741e-01, -2.5701e-01,  1.4082e-01, -4.8859e-01,  3.6192e-01,\n",
      "          7.9369e-02, -2.4549e-01, -1.0040e-01,  4.8873e-01,  1.6360e-01,\n",
      "          1.8831e-01,  2.0683e-02,  8.1469e-01, -3.0241e-01, -4.5968e-01,\n",
      "         -3.4863e-01,  1.2450e-01,  1.6109e-01, -2.5407e-01,  4.8373e-01,\n",
      "          7.7841e-01,  4.0809e-01,  3.1923e-01,  3.8188e-01, -5.0518e-01,\n",
      "          1.4122e-01, -2.9783e-01,  1.1943e-01,  2.0137e-01, -6.9425e-01,\n",
      "          2.7428e-01,  4.4774e-01, -3.6416e-01,  4.9342e-01,  3.0496e-02,\n",
      "          3.9638e-01,  3.8552e-01, -9.6225e-01,  7.1589e-01,  7.3672e-01,\n",
      "         -1.1004e+00,  3.4675e-02,  5.2235e-02, -2.1578e-01,  2.2747e-01,\n",
      "         -4.1360e-01, -6.7706e-01, -5.2209e-01,  2.1182e-01,  5.0830e-01,\n",
      "          6.6081e-01,  8.6568e-02,  3.9309e-01,  2.3277e-01, -2.6182e-01,\n",
      "         -8.2236e-01, -8.9240e-01, -3.1702e-01,  7.6746e-01, -7.4214e-01,\n",
      "         -5.5288e-01, -6.1008e-01, -5.4594e-01, -1.1861e+00, -5.1617e-01,\n",
      "         -8.1437e-02,  9.5771e-01,  8.4769e-01, -1.9447e-01,  2.4564e-01,\n",
      "          1.0551e+00, -2.3276e-01, -1.9916e-01, -4.2484e-01, -1.3345e+00,\n",
      "         -6.8116e-01, -1.0295e+00, -2.8051e-01, -7.9018e-01, -7.3602e-01,\n",
      "         -7.1630e-01, -8.5521e-01, -1.2249e+00, -2.4311e-02,  1.9448e-01,\n",
      "         -1.7682e+00, -5.7672e-01, -5.8767e-01, -4.8261e-01, -9.8057e-01,\n",
      "         -6.1907e-01,  2.2962e-01, -6.7192e-01, -1.1619e+00, -3.9890e-01,\n",
      "          6.8021e-01, -1.8477e-01, -2.2133e-01,  1.2078e-01,  6.6934e-01,\n",
      "         -5.8970e-01, -7.2066e-01, -9.9239e-01, -1.1364e+00, -6.6107e-01,\n",
      "         -1.4359e+00, -1.0241e+00, -1.3064e+00, -1.6532e+00, -1.3931e+00,\n",
      "         -1.5588e+00, -1.1844e+00,  3.2270e-01, -1.7238e-01, -2.0773e-01,\n",
      "          2.1745e-01,  1.6016e-01, -1.3524e-01,  2.9703e-01, -2.5215e-01,\n",
      "         -4.5510e-01, -1.5672e+00,  5.1032e-01,  7.7583e-01, -1.1491e+00,\n",
      "         -2.1437e-01,  9.6760e-01, -3.2835e-02, -9.4947e-01, -6.6429e-01,\n",
      "          9.0070e-01, -3.0274e-01, -1.4185e+00,  2.1094e-01, -9.8939e-01,\n",
      "         -8.3424e-01, -1.7505e+00, -1.1111e+00, -4.8577e-01, -1.0218e+00,\n",
      "          4.8135e-01,  1.1799e+00,  1.8517e-01,  6.5806e-01,  5.7712e-01,\n",
      "          1.3503e-01,  5.6471e-01,  3.1784e-01,  4.6121e-01, -2.1095e-01,\n",
      "         -7.8421e-01, -1.2713e+00, -5.7531e-01, -1.0712e+00, -9.2549e-01,\n",
      "         -6.3703e-01, -1.1008e-01, -5.0939e-01,  1.5195e-02, -1.8630e-01,\n",
      "         -8.1365e-01, -1.0013e+00,  5.8064e-01, -3.8978e-01, -6.4569e-01,\n",
      "          3.5401e-01, -5.0940e-01, -2.9165e-01, -5.5819e-01, -6.6229e-01,\n",
      "         -6.4121e-01, -1.2531e+00, -1.0765e+00, -1.0281e+00, -5.9287e-02,\n",
      "          1.1312e+00,  2.5093e-01, -1.3046e+00, -1.6994e+00,  9.8591e-02,\n",
      "          5.9690e-01, -1.0734e+00, -5.6667e-01,  3.1828e-01,  1.1901e-01,\n",
      "         -1.3233e+00,  1.1385e-01,  6.1818e-02, -2.2567e+00, -1.8417e+00,\n",
      "         -7.4191e-01,  1.1644e-01, -4.0798e-01, -2.7787e-01,  8.5904e-01,\n",
      "         -4.3004e-02,  2.6264e-01,  2.0047e+00,  7.2268e-01,  1.7977e-02,\n",
      "          8.5138e-01, -1.7529e-01,  1.6460e-01,  1.8364e-01,  8.8008e-01,\n",
      "          6.7830e-01,  1.0825e+00, -1.5512e-01,  1.9448e-02, -1.5458e-01,\n",
      "         -1.1165e+00, -2.3304e-02,  1.7068e+00,  2.0495e+00,  1.2810e-01,\n",
      "         -1.1168e+00, -6.6735e-03,  1.0276e-01,  3.6111e-01,  7.4329e-01,\n",
      "          1.1780e+00, -5.5518e-01, -6.2547e-01,  7.5755e-01,  1.8884e-01,\n",
      "          8.5949e-01,  4.8430e-01, -1.4608e-01, -8.3891e-01, -4.4671e-01,\n",
      "          9.9638e-02,  2.7342e-01,  1.4541e+00,  8.5260e-01, -9.6265e-01,\n",
      "         -2.3933e-01,  6.5234e-01,  3.0352e-01, -4.6832e-01, -4.8679e-01,\n",
      "          6.9975e-01,  1.4336e+00,  8.9918e-01,  5.1219e-03,  5.4420e-01,\n",
      "         -9.0803e-01,  2.9467e-01,  1.2088e+00,  2.6179e+00,  1.1803e+00,\n",
      "         -3.9135e-01, -1.5332e+00,  9.1387e-02, -3.7392e-01,  1.7388e+00,\n",
      "          1.3621e+00,  4.2427e-01,  1.5756e-01,  1.1481e+00, -5.5312e-01,\n",
      "          5.0955e-02,  1.2007e-01,  3.8819e-01,  5.9559e-01,  4.9846e-01,\n",
      "         -1.4076e-01,  4.2027e-03,  1.1890e-01, -8.1983e-01, -1.3886e+00,\n",
      "         -2.1874e-01, -9.9771e-01,  1.2370e+00,  2.0039e+00,  1.1192e+00,\n",
      "         -6.2739e-03,  8.5744e-01,  7.1728e-01, -1.1037e+00,  1.2401e+00,\n",
      "         -1.0754e+00, -2.7223e-02, -8.0290e-01, -5.2596e-01,  1.4160e+00,\n",
      "         -1.5564e+00,  3.4192e-01,  1.1416e+00,  6.5956e-01,  9.6197e-01,\n",
      "          1.1497e+00,  1.0609e+00,  7.2448e-01,  3.2713e-01,  3.8435e-01,\n",
      "         -1.3345e+00, -1.0352e+00,  1.0034e+00,  1.0859e-01,  1.0553e+00,\n",
      "          1.6952e+00,  3.0661e-01,  1.6631e-02,  1.2666e+00,  4.2937e-01,\n",
      "         -9.8695e-01,  2.7844e-01,  8.1282e-01,  1.5069e+00,  3.5696e-02,\n",
      "         -4.8794e-01, -3.7976e-01, -7.4662e-01,  3.0100e-01, -1.1581e-01,\n",
      "          7.5366e-01,  2.1834e-01, -4.8490e-02, -1.2028e+00,  4.8589e-01,\n",
      "         -6.5050e-01, -5.5257e-01, -8.3270e-01, -7.8143e-02,  1.0800e+00,\n",
      "         -1.3716e+00,  1.3113e+00,  6.6099e-01,  1.0663e+00,  5.0569e-01,\n",
      "          5.8554e-01,  3.4639e-01, -2.1410e+00, -1.3875e+00, -2.2584e-01,\n",
      "         -6.0920e-01, -1.3110e-01,  6.7617e-01, -2.6863e-01, -1.6936e+00,\n",
      "         -9.8719e-01,  4.5621e-01,  2.3853e-01,  1.0942e+00,  7.4949e-01,\n",
      "         -4.8115e-01, -1.8306e-01,  1.2041e+00,  2.5699e-02, -1.2559e+00,\n",
      "         -1.1045e+00,  2.6036e-01,  1.2163e+00,  2.2238e-01, -5.9069e-01,\n",
      "          1.0498e+00, -1.5422e-01,  1.1678e+00, -9.2623e-01,  3.4003e-01,\n",
      "         -2.4595e-01, -1.1164e+00,  1.1886e+00,  4.3129e-01,  1.2722e-01,\n",
      "          4.8810e-01, -1.4942e-01,  7.5736e-01,  6.5568e-01,  1.1509e+00,\n",
      "          6.9589e-01, -3.2997e-01,  1.5620e+00,  8.3212e-01,  1.4105e+00,\n",
      "         -5.6662e-01,  5.2548e-01, -7.2373e-01,  7.2728e-01,  1.3229e-01,\n",
      "         -4.0468e-01,  9.3021e-01, -4.2827e-01, -7.0290e-01,  1.0407e+00,\n",
      "          2.3570e+00, -2.2678e-01, -2.5960e-01, -6.0808e-01,  6.0162e-01,\n",
      "         -1.4482e-01,  1.3323e+00, -4.9143e-01,  4.7743e-01, -1.4742e-01,\n",
      "          6.8953e-01,  5.7170e-01, -8.3911e-01,  5.3945e-01, -1.1888e-01,\n",
      "         -2.1215e-02,  1.1362e+00,  5.7447e-01,  1.9664e+00,  1.0845e+00,\n",
      "          8.0281e-01,  8.5313e-01,  3.2524e-01,  7.0284e-01, -1.6898e-01,\n",
      "         -1.4337e+00,  1.1087e+00, -6.0196e-01, -1.3953e+00,  3.7767e-01,\n",
      "         -2.1094e-01,  8.2783e-01,  3.6730e-01,  1.4670e+00, -2.6398e-01,\n",
      "          5.1898e-01,  1.0212e+00,  6.8201e-01,  5.0296e-01,  2.1545e-01,\n",
      "         -1.7246e+00,  1.1136e+00, -5.7922e-01,  1.2425e+00,  5.1832e-01,\n",
      "         -1.1838e+00,  6.4079e-01,  6.0774e-01, -5.6398e-01, -1.3505e+00,\n",
      "          7.8403e-01, -4.4372e-02,  6.3891e-01,  1.0572e+00, -4.1447e-01,\n",
      "          4.8557e-01,  2.7444e-02, -1.6659e-02, -2.2596e-01,  6.3985e-01,\n",
      "         -2.0599e-01, -1.2799e+00, -3.6252e-01, -9.8100e-01,  7.5514e-01,\n",
      "         -7.2139e-02,  1.3225e+00, -2.5940e-02, -1.1507e+00, -4.5954e-01,\n",
      "          2.2004e-01, -1.5746e-02, -5.5319e-01,  4.9070e-01,  1.3898e+00,\n",
      "         -9.9208e-01,  1.5370e+00,  1.4508e+00,  6.8763e-01,  2.8201e-01,\n",
      "          5.8914e-01,  2.8797e-01, -6.7200e-01,  4.7645e-01,  7.2979e-01,\n",
      "         -1.5827e+00, -6.6594e-02, -1.3677e+00, -3.7102e-01, -9.3987e-01,\n",
      "         -5.1516e-01,  5.6943e-01,  8.0385e-01,  5.5986e-01, -1.0522e+00,\n",
      "          9.2624e-01,  1.7207e+00,  1.1611e-01, -6.5511e-01,  2.6603e-01,\n",
      "          2.0129e+00, -3.1911e-01, -4.2600e-01,  4.0958e-01,  5.9167e-01,\n",
      "         -6.4565e-01, -4.6165e-01, -1.0120e-02,  8.3943e-01, -2.7114e-02,\n",
      "          8.6711e-01,  1.1942e+00,  1.7023e-01, -6.7746e-01,  6.2364e-01,\n",
      "         -3.1173e-01,  6.1190e-01, -8.5945e-01, -3.4748e-01,  5.3733e-01,\n",
      "         -2.9881e-02,  1.6061e-01,  1.2901e+00,  1.0530e-01, -9.2055e-01,\n",
      "          1.5464e+00, -5.5541e-01, -4.5057e-01,  1.4037e+00, -1.7320e-01,\n",
      "          5.2777e-01,  1.9667e+00, -8.0292e-01,  1.6517e+00, -1.6582e+00,\n",
      "         -4.7738e-02, -3.1653e-01,  8.3918e-01,  1.0557e+00,  1.5630e-01,\n",
      "          1.0427e+00, -2.8764e-01,  3.6501e-01, -2.9158e-02,  6.2807e-01,\n",
      "          4.3096e-02,  1.6858e-02,  5.1787e-01,  7.4674e-01,  1.5925e+00,\n",
      "          6.0986e-01, -3.0500e-01, -3.5668e-02,  7.1414e-01,  5.8282e-01,\n",
      "         -5.3566e-01,  1.1706e+00, -4.4285e-01,  1.3351e+00, -2.6457e-01,\n",
      "         -7.7384e-02,  6.9941e-01,  3.7451e-01,  6.0467e-01,  1.1741e+00,\n",
      "          6.3084e-01,  4.8056e-01,  4.2367e-01, -5.4495e-01,  1.3460e+00,\n",
      "          1.5696e-01,  5.4233e-01,  1.6105e+00,  6.7325e-01,  7.3637e-01,\n",
      "          3.6375e-01,  4.2079e-01,  5.2393e-01,  1.5082e+00, -1.0689e+00,\n",
      "         -1.2557e+00, -7.6838e-01,  7.2245e-01,  1.2003e+00,  1.5161e+00,\n",
      "          1.2931e-01,  6.0664e-01,  9.9927e-01,  3.0162e-01, -5.9167e-02,\n",
      "          4.6846e-01,  9.6148e-01,  1.7007e+00,  9.1438e-01,  7.0602e-01,\n",
      "         -7.5033e-02,  8.8366e-01,  7.2903e-01, -9.4027e-01,  2.5695e-01,\n",
      "         -1.0775e+00,  1.8220e-01, -7.7262e-01, -1.0132e+00,  1.4928e+00,\n",
      "          1.0646e+00,  3.5022e-02,  1.7945e-01,  1.4664e+00,  8.0245e-02,\n",
      "         -2.1063e-01,  9.5512e-01, -2.6185e-01,  1.7992e+00, -1.2996e+00,\n",
      "         -1.9711e-02,  1.0433e-01, -1.3062e+00,  1.8541e+00,  2.2572e-01,\n",
      "         -1.8012e+00, -1.1085e+00,  2.4260e-01,  8.5361e-01,  6.6824e-01,\n",
      "         -6.3232e-01,  1.9757e-01,  1.1067e+00,  1.5083e+00, -9.1460e-01,\n",
      "          8.6608e-01,  3.7739e-01, -7.9100e-01, -9.7687e-01, -1.8906e-01,\n",
      "          6.6319e-01,  1.5831e+00,  1.4957e+00,  8.0146e-01, -6.4838e-01,\n",
      "          1.2156e+00,  4.5468e-01,  1.6060e-01,  5.2254e-01,  8.0221e-01,\n",
      "          1.5802e+00,  9.0569e-01, -2.8138e-01,  9.7443e-02,  9.1263e-01,\n",
      "          9.8815e-01,  1.2179e+00,  1.7805e+00, -6.3829e-01, -5.2893e-01,\n",
      "          9.4538e-01, -6.5825e-01, -1.1738e-01, -7.7671e-01,  9.3968e-01,\n",
      "          2.0697e-01,  1.4005e+00,  9.8239e-01, -2.3457e-01, -5.7722e-01,\n",
      "          5.5778e-01,  7.8269e-02, -3.9794e-01,  1.7530e+00, -5.5940e-01,\n",
      "          9.4975e-01, -1.5560e+00,  7.6261e-01, -1.2586e+00, -2.6694e+00,\n",
      "          8.0559e-02,  1.6419e+00, -4.8967e-01, -2.0506e-02,  1.3120e+00,\n",
      "          1.0562e+00, -6.7901e-01,  8.2264e-01,  1.5047e+00, -2.0684e-02,\n",
      "          5.7691e-01, -4.0988e-01, -4.0233e-01, -1.1947e+00, -1.6916e-01,\n",
      "         -8.6622e-01,  4.2156e-01,  7.2795e-01,  5.9753e-02, -9.1719e-01,\n",
      "         -6.4193e-01,  1.3744e+00,  5.8513e-01,  1.9819e+00,  2.0278e+00,\n",
      "         -1.2836e+00, -3.5163e-01,  1.5203e+00,  7.6754e-01,  9.9152e-01,\n",
      "          1.9348e-01, -6.0398e-01,  1.2335e+00, -1.0490e+00,  1.1502e+00,\n",
      "          1.4114e+00,  9.9379e-01,  1.0933e+00, -3.3814e-01, -2.0750e+00,\n",
      "         -7.8757e-01,  1.3831e-01,  1.7852e-01,  1.7087e-01,  1.8820e-01,\n",
      "         -3.0504e-01,  1.1759e+00, -7.5715e-01,  6.1556e-01, -5.7243e-02,\n",
      "         -7.5543e-01, -7.1490e-01, -4.0506e-01, -7.0465e-02,  1.3411e+00,\n",
      "         -2.4509e-01, -9.4725e-03,  3.2931e-01, -1.6563e+00, -2.3029e-01,\n",
      "         -3.3440e-01,  4.6680e-01,  1.8721e-01,  2.7869e-02, -2.8018e-02,\n",
      "         -1.8288e-01, -5.1173e-01,  3.5597e-02,  4.3435e-01, -3.4525e-01,\n",
      "         -5.4004e-01, -1.1192e+00,  7.2134e-01,  9.1024e-01, -2.9032e-01,\n",
      "          1.0493e-01, -3.3617e-01, -2.9680e-01,  2.1803e-01,  8.4207e-01,\n",
      "         -5.7628e-01, -4.4719e-01, -4.9321e-01,  1.9925e-01, -1.1537e+00,\n",
      "          2.6829e-01,  6.8240e-01, -1.9331e-01, -8.2979e-01, -1.2592e+00,\n",
      "          1.3408e-01,  5.2948e-01, -7.3551e-01,  9.1479e-01,  1.8992e-01,\n",
      "          4.0686e-02,  7.6948e-01, -3.0312e-01, -5.8636e-02, -2.1270e+00,\n",
      "          1.0811e+00, -1.7051e+00,  4.8301e-01,  1.9575e-01, -6.2416e-01,\n",
      "         -8.2343e-01, -8.5919e-02,  4.8935e-01, -5.0002e-01, -7.4755e-01,\n",
      "         -1.4142e+00, -2.2160e+00,  1.7289e+00, -1.6706e-01, -6.4891e-01,\n",
      "         -4.2156e-01, -9.3825e-01, -6.3474e-01, -1.7313e+00, -4.3008e-01,\n",
      "         -1.4908e-01,  4.8939e-01, -2.9993e-01,  1.6180e+00,  1.0959e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "prediction = model(data) \n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0d3a75f08930bd918b64f150ed3b1897edcb5adbd0b50636870c9fa7bcf7046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
